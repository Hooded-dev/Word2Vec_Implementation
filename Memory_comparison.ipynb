{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib\n",
    "import re,sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pympler.asizeof import asizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 80 344\n",
      "b: 104 160\n"
     ]
    }
   ],
   "source": [
    "a=[[1,0,0,1,0],[0,0,1,1,0]]\n",
    "b = [1,0,1,1,0]\n",
    "print('a: %s %s' %(sys.getsizeof(a) , asizeof(a)))\n",
    "print('b: %s %s' %(sys.getsizeof(b) , asizeof(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = []\n",
    "with open('text.txt') as f:\n",
    "        file_contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for val in file_contents.split('.'):\n",
    "    sent = re.findall(\"[A-Za-z]+\", val)\n",
    "    text.append(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inital_values(text):\n",
    "    word_to_index= dict()\n",
    "    index_to_word = dict()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    vocab_size = 0\n",
    "    \n",
    "    for row in text:\n",
    "        for word in row.split():\n",
    "            word = word.lower()\n",
    "            corpus.append(word)\n",
    "            if word_to_index.get(word) == None:\n",
    "                word_to_index.update ( {word : count})\n",
    "                index_to_word.update ( {count : word })\n",
    "                count  += 1\n",
    "    vocab_size = len(word_to_index)\n",
    "    length_of_corpus = len(corpus)\n",
    "    \n",
    "    return word_to_index,index_to_word,corpus,length_of_corpus,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_multiple_vectors(target_word,context_words):\n",
    "    trgt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    index_of_word_dictionary = word_to_index.get(target_word) \n",
    "    trgt_word_vector[index_of_word_dictionary] = 1\n",
    "    \n",
    "    ctxt_word_vector = []\n",
    "    \n",
    "    \n",
    "    for word in context_words:\n",
    "        ctxt_word_vector_temp = np.zeros(vocab_size)\n",
    "        index_of_word_dictionary = word_to_index.get(word) \n",
    "        ctxt_word_vector_temp[index_of_word_dictionary] = 1\n",
    "        \n",
    "        ctxt_word_vector.append(ctxt_word_vector_temp)\n",
    "        \n",
    "    \n",
    "    return trgt_word_vector,ctxt_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_single_vectors(target_word,context_words):\n",
    "    trgt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    index_of_word_dictionary = word_to_index.get(target_word) \n",
    "    trgt_word_vector[index_of_word_dictionary] = 1\n",
    "    \n",
    "    ctxt_word_vector = np.zeros(vocab_size)\n",
    "    \n",
    "    \n",
    "    for word in context_words:\n",
    "        index_of_word_dictionary = word_to_index.get(word) \n",
    "        ctxt_word_vector[index_of_word_dictionary] = 1\n",
    "        \n",
    "    return trgt_word_vector,ctxt_word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(corpus,length_of_corpus,window_size,method):\n",
    "    training_data =  []\n",
    "    for i,word in enumerate(corpus):\n",
    "        \n",
    "        index_target_word = i\n",
    "        target_word = word\n",
    "        context_words = []\n",
    "        \n",
    "        if i == 0:  \n",
    "            context_words = [corpus[x] for x in range(i + 1 , window_size + 1)] \n",
    "\n",
    "        elif i == len(corpus)-1:\n",
    "            context_words = [corpus[x] for x in range(length_of_corpus - 2 ,length_of_corpus -2 - window_size  , -1 )]\n",
    "\n",
    "        else:\n",
    "            before_target_word_index = index_target_word - 1\n",
    "            for x in range(before_target_word_index, before_target_word_index - window_size , -1):\n",
    "                if x >=0:\n",
    "                    context_words.extend([corpus[x]])\n",
    "\n",
    "            after_target_word_index = index_target_word + 1\n",
    "            for x in range(after_target_word_index, after_target_word_index + window_size):\n",
    "                if x < len(corpus):\n",
    "                    context_words.extend([corpus[x]])\n",
    "        if method == 'single':\n",
    "            trgt_word_vector,ctxt_word_vector = get_one_hot_single_vectors(target_word,context_words)\n",
    "        elif method == 'multiple':\n",
    "            trgt_word_vector,ctxt_word_vector = get_one_hot_multiple_vectors(target_word,context_words)\n",
    "        \n",
    "        training_data.append([trgt_word_vector,ctxt_word_vector])\n",
    "    return training_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index,index_to_word,corpus,length_of_corpus,vocab_size = get_inital_values(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 449\n",
      "Length of corpus : 845\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words:' , vocab_size)\n",
    "#print('word_to_index : ',word_to_index)\n",
    "#print('index_to_word : ',index_to_word)\n",
    "#print('corpus:',corpus)\n",
    "print('Length of corpus :',length_of_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Method 1 : multiple arrays for context word </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_data(corpus,length_of_corpus,window_size,'multiple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : 33599.3203 kbs\n"
     ]
    }
   ],
   "source": [
    "print('Size : %.4f kbs' %(asizeof(training_data)/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Method 2 : single arrays for context word </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_data(corpus,length_of_corpus,window_size,'single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size : 6159.5469 kbs\n"
     ]
    }
   ],
   "source": [
    "print('Size : %.4f kbs' %(asizeof(training_data)/1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
